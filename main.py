# Suppress external library logs first
import utils.suppress_logs

# Import logger after suppressing external logs
from utils.logger import logger, set_log_level, LogLevel

from preprocess.dataset import knowledge_info
from preprocess.stat_info_functions import (
    stat_info_collection,
    convert_stat_info_to_text,
)
from causal_discovery.filter import Filter
from causal_discovery.program import Programming
from causal_discovery.rerank import Reranker
from causal_discovery.hyperparameter_selector import HyperparameterSelector
from postprocess.judge import Judge
from postprocess.visualization import Visualization, convert_to_edges
from preprocess.eda_generation import EDA
from report.report_generation import Report_generation
from global_setting.Initialize_state import global_state_initialization, load_data

import os
import json
import argparse
import numpy as np
import pandas as pd
from dotenv import load_dotenv

load_dotenv()
set_log_level(LogLevel.DEBUG)


def parse_args():
    parser = argparse.ArgumentParser(
        description="Causal Learning Tool for Data Analysis"
    )

    # Input data file
    parser.add_argument(
        "--data-file",
        type=str,
        # default= "simulated_data/default/data.csv",
        default="data/dataset/Abalone/Abalone.csv",
        help="Path to the input dataset file (e.g., CSV format or directory location). If file, supports csv and json format.",
    )

    # Output file for results
    parser.add_argument(
        "--output-report-dir",
        type=str,
        # default='data/dataset/sim_ts/output_report/',
        default="output/Abalone",
        help="Directory to save the output report",
    )

    # Output directory for graphs
    parser.add_argument(
        "--output-graph-dir",
        type=str,
        # default='data/dataset/sim_ts/output_graph/',
        default="output/Abalone",
        help="Directory to save the output graph",
    )

    parser.add_argument(
        "--simulation_mode",
        type=str,
        default="offline",
        help="Simulation mode: online or offline. If online, the simulation manager generate data when loading data.",
    )

    parser.add_argument(
        "--data_mode", type=str, default="real", help="Data mode: real or simulated"
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        default=False,
        help="Enable debugging mode, ugly logic to use fake data in debug mode!",
    )

    parser.add_argument(
        "--initial_query",
        type=str,
        default="Do causal discovery on this dataset",
        help="Initial query for the algorithm",
    )

    parser.add_argument(
        "--parallel",
        type=bool,
        default=False,
        help="Parallel computing for bootstrapping.",
    )

    parser.add_argument(
        "--demo_mode", type=bool, default=False, help="Demo mode, affect the saved dir."
    )

    args = parser.parse_args()
    return args


def load_real_world_data(file_path):
    # Baseline code
    # Checking file format and loading accordingly, right now it's for CSV only
    if file_path.endswith(".csv"):
        data = pd.read_csv(file_path)
    elif file_path.endswith(".json"):
        with open(file_path, "r") as f:
            data = pd.DataFrame(json.load(f))
    else:
        raise ValueError(f"Unsupported file format for {file_path}")

    # Show basic dataset information
    data_info = {
        "Shape": f"({data.shape[0]:,} rows, {data.shape[1]} columns)",
        "Columns": f"{list(data.columns[:5]) = }{'...' if len(data.columns) > 5 else ''}",
        "Memory usage": f"{data.memory_usage(deep=True).sum() / 1024**2:.1f} MB",
        "Data types": f"{data.dtypes.value_counts().to_dict()}",
    }
    logger.data_info("Dataset loaded successfully", data_info)
    return data


def process_user_query(query, data):
    logger.detail(f"Processing user query: {query[:100]}...")

    # Baseline code
    query_dict = {}
    original_shape = data.shape

    if ";" in query or ":" in query:
        for part in query.split(";"):
            if ":" in part:
                key, value = part.strip().split(":")
                query_dict[key.strip()] = value.strip()

    if "filter" in query_dict and query_dict["filter"] == "continuous":
        # Filtering continuous columns, just for target practice right now
        data = data.select_dtypes(include=["float64", "int64"])
        logger.detail(
            f"Filtered to continuous columns: {original_shape} â†’ {data.shape}"
        )

    if "selected_algorithm" in query_dict:
        selected_algorithm = query_dict["selected_algorithm"]
        logger.algorithm("Algorithm manually selected", selected_algorithm)

    # Show query processing results
    processing_results = {
        "Original query": query[:50] + "..." if len(query) > 50 else query,
        "Parsed parameters": len(query_dict),
        "Data shape after processing": f"{data.shape}",
        "Columns selected": f"{len(data.columns)} columns",
    }
    logger.data_info("User query processed", processing_results)
    return data


def main(args):
    logger.header("Causal-Copilot Analysis Session")

    logger.step(1, 8, "Initializing global state")
    global_state = global_state_initialization(args)
    logger.detail("Global state initialized successfully")

    logger.step(2, 8, "Loading and preparing data")
    global_state = load_data(global_state, args)
    logger.detail("Data loading completed")

    if args.data_mode == "real":
        global_state.user_data.raw_data = load_real_world_data(args.data_file)

    logger.step(3, 8, "Processing user query")
    global_state.user_data.processed_data = process_user_query(
        args.initial_query, global_state.user_data.raw_data
    )
    global_state.user_data.visual_selected_features = (
        global_state.user_data.processed_data.columns.tolist()
    )
    global_state.user_data.selected_features = (
        global_state.user_data.processed_data.columns.tolist()
    )

    logger.step(4, 8, "Collecting statistical information")
    if args.debug:
        logger.detail("Using debug mode with fake statistics")
        # Fake statistics for debugging
        global_state.statistics.sample_size = 853
        global_state.statistics.feature_number = 11
        global_state.statistics.missingness = False
        global_state.statistics.data_type = "Continuous"
        global_state.statistics.linearity = True
        global_state.statistics.gaussian_error = True
        global_state.statistics.stationary = "non time-series"
        global_state.user_data.processed_data = global_state.user_data.raw_data
        global_state.user_data.knowledge_docs = (
            "This is fake domain knowledge for debugging purposes."
        )
        logger.detail("Debug statistics and knowledge information loaded")
    else:
        logger.detail("Analyzing dataset characteristics...")
        global_state = stat_info_collection(global_state)
        logger.detail("Collecting domain knowledge...")
        global_state = knowledge_info(args, global_state)

    # Convert statistics to text
    global_state.statistics.description = convert_stat_info_to_text(
        global_state.statistics
    )

    # Show detailed data information
    data_details = {
        "Shape": f"{global_state.user_data.processed_data.shape}",
        "Columns": len(global_state.user_data.processed_data.columns),
        "Missing values": global_state.user_data.processed_data.isnull().sum().sum(),
        "Data type": getattr(global_state.statistics, "data_type", "Unknown"),
    }
    logger.data_info("Dataset preprocessed", data_details)

    logger.checkpoint("Data preprocessing completed")

    #############EDA###################
    logger.step(5, 8, "Exploratory Data Analysis")
    logger.detail("Generating statistical summaries and visualizations...")
    my_eda = EDA(global_state)
    my_eda.generate_eda()
    logger.detail("EDA completed - visualizations saved")

    logger.step(6, 8, "Algorithm Selection")

    logger.detail("Step 1/3: Filtering suitable algorithms")
    filter = Filter(args)
    global_state = filter.forward(global_state)
    if hasattr(global_state.algorithm, "filtered_algorithms"):
        logger.detail(
            f"Filtered to {len(global_state.algorithm.filtered_algorithms)} candidate algorithms"
        )
    else:
        logger.detail("Algorithm filtering completed")

    logger.detail("Step 2/3: Ranking algorithms by suitability")
    reranker = Reranker(args)
    global_state = reranker.forward(global_state)
    logger.detail("Algorithm ranking completed")

    logger.detail("Step 3/3: Optimizing hyperparameters")
    hp_selector = HyperparameterSelector(args)
    global_state = hp_selector.forward(global_state)

    logger.algorithm("Selected algorithm", global_state.algorithm.selected_algorithm)
    if hasattr(global_state.algorithm, "hyperparameters"):
        logger.detail(
            f"Hyperparameters: {len(global_state.algorithm.hyperparameters)} parameters optimized"
        )
    else:
        logger.detail("Hyperparameter optimization completed")

    logger.step(7, 8, "Algorithm Execution")
    logger.detail(f"Running {global_state.algorithm.selected_algorithm} algorithm...")
    try:
        programmer = Programming(args)
        global_state = programmer.forward(global_state)
        logger.detail("Algorithm execution completed")

        # Show graph statistics
        if hasattr(global_state.results, "converted_graph"):
            graph = global_state.results.converted_graph
            if graph is not None:
                edges = (graph != 0).sum()
                logger.detail(f"Discovered {edges} edges in causal graph")
            else:
                logger.warning("No graph result found")
        else:
            logger.warning("No results attribute found")

        logger.checkpoint("Causal discovery completed")
    except Exception as e:
        logger.error(f"Algorithm execution failed: {str(e)}")
        raise

    #############Visualization for Initial Graph###################
    my_visual_initial = Visualization(global_state)
    if (
        global_state.statistics.time_series
        and global_state.results.lagged_graph is not None
    ):
        converted_graph = global_state.results.lagged_graph
        pos_est = my_visual_initial.get_pos(converted_graph[0])
        for i in range(converted_graph.shape[0]):
            _ = my_visual_initial.plot_pdag(
                converted_graph[i],
                f"{global_state.algorithm.selected_algorithm}_initial_graph_{i}.svg",
                pos=pos_est,
            )
        summary_graph = np.any(converted_graph, axis=0).astype(int)
        # pos_est = my_visual_initial.get_pos(summary_graph)
        _ = my_visual_initial.plot_pdag(
            summary_graph,
            f"{global_state.algorithm.selected_algorithm}_initial_graph_summary.svg",
            pos=pos_est,
        )
        my_report = Report_generation(global_state, args)
    else:
        # Get the position of the nodes
        pos_est = my_visual_initial.get_pos(global_state.results.converted_graph)
        # Plot True Graph
        if global_state.user_data.ground_truth is not None:
            _ = my_visual_initial.plot_pdag(
                global_state.user_data.ground_truth, "true_graph.pdf", pos=pos_est
            )
        # Plot Initial Graph
        _ = my_visual_initial.plot_pdag(
            global_state.results.converted_graph,
            f"{global_state.algorithm.selected_algorithm}_initial_graph.pdf",
            pos=pos_est,
        )
        my_report = Report_generation(global_state, args)
        global_state.results.raw_edges = convert_to_edges(
            global_state.algorithm.selected_algorithm,
            global_state.user_data.processed_data.columns,
            global_state.results.converted_graph,
        )
        global_state.logging.graph_conversion["initial_graph_analysis"] = (
            my_report.graph_effect_prompts()
        )
        judge = Judge(global_state, args)
        if global_state.user_data.ground_truth is not None:
            logger.section("Graph Evaluation")
            logger.detail("Comparing with ground truth graph")
            global_state.results.metrics = judge.evaluation(global_state)
            if (
                hasattr(global_state.results, "metrics")
                and global_state.results.metrics
            ):
                logger.metrics_table(
                    global_state.results.metrics, "Performance Metrics"
                )

        logger.section("Graph Refinement")
        logger.detail("Applying bootstrap sampling and statistical tests")
        global_state = judge.forward(global_state, "cot_all_relation", 1)
        logger.success("Graph refinement completed")

    #############Visualization for Revised Graph###################
    logger.section("Graph Visualization")
    logger.detail("Generating visualization for revised graph and confidence heatmaps")

    # Plot Revised Graph
    my_visual_revise = Visualization(global_state)
    pos_new = my_visual_revise.plot_pdag(
        global_state.results.revised_graph,
        f"{global_state.algorithm.selected_algorithm}_revised_graph.pdf",
        pos=pos_est,
    )
    global_state.results.revised_edges = convert_to_edges(
        global_state.algorithm.selected_algorithm,
        global_state.user_data.processed_data.columns,
        global_state.results.revised_graph,
    )

    # Plot Bootstrap Heatmap - CRITICAL: This must happen before report generation
    logger.detail("Generating bootstrap confidence heatmaps")
    boot_heatmap_paths = my_visual_revise.boot_heatmap_plot()
    if boot_heatmap_paths:
        logger.success(f"Generated {len(boot_heatmap_paths)} confidence heatmap(s)")
        for path in boot_heatmap_paths:
            logger.debug(
                f"Generated heatmap: {os.path.basename(path)}", "Visualization"
            )
    else:
        logger.warning(
            "No confidence heatmaps were generated (bootstrap data may be empty)"
        )

    # global_state.results.refutation_analysis = judge.graph_refutation(global_state)

    # algorithm selection process
    """
    round = 0
    flag = False

    while round < args.max_iterations and flag == False:
        code, results = programmer.forward(preprocessed_data, algorithm, hyper_suggest)
        flag, algorithm_setup = judge(preprocessed_data, code, results, statistics_dict, algorithm_setup, knowledge_docs)
    """
    logger.step(8, 8, "Report Generation")
    #############Report Generation###################
    try_num = 1

    logger.detail("Step 1/3: Analyzing causal relationships")
    try:
        global_state.results.raw_edges = convert_to_edges(
            global_state.algorithm.selected_algorithm,
            global_state.user_data.processed_data.columns,
            global_state.results.converted_graph,
        )
        global_state.logging.graph_conversion["initial_graph_analysis"] = (
            my_report.graph_effect_prompts()
        )
        analysis_clean = (
            global_state.logging.graph_conversion["initial_graph_analysis"]
            .replace('"', "")
            .replace("\\n\\n", "\n\n")
            .replace("\\n", "\n")
            .replace("'", "")
        )
        logger.detail("Causal relationship analysis completed")
    except Exception as e:
        logger.error(f"Causal relationship analysis failed: {str(e)}")
        raise

    logger.detail("Step 2/3: Generating comprehensive report")
    try:
        my_report = Report_generation(global_state, args)
        report = my_report.generation()
        my_report.save_report(report)
        report_path = os.path.join(
            global_state.user_data.output_report_dir, "report.pdf"
        )

        while (not os.path.isfile(report_path)) and try_num < 3:
            try_num += 1
            logger.warning(f"Report generation failed, retrying ({try_num}/3)")
            report_gen = Report_generation(global_state, args)
            report = report_gen.generation(debug=False)
            report_gen.save_report(report)

        if os.path.isfile(report_path):
            logger.detail("Step 3/3: Report saved successfully")
            logger.detail(f"Report location: {os.path.basename(report_path)}")

            # Show final summary
            final_summary = {
                "Algorithm used": global_state.algorithm.selected_algorithm,
                "Graph edges": f"{(global_state.results.converted_graph != 0).sum() if hasattr(global_state.results, 'converted_graph') and global_state.results.converted_graph is not None else 'Unknown'}",
                "Report saved": os.path.basename(report_path),
                "Output directory": os.path.basename(
                    global_state.user_data.output_report_dir
                ),
            }
            logger.data_info("Analysis completed successfully", final_summary)
        else:
            logger.error("Report generation failed after 3 attempts")
    except Exception as e:
        logger.error(f"Report generation failed: {str(e)}")
        raise
    ################################

    # User discussion part
    from user.discuss import Discussion

    discussion = Discussion(args, report)
    discussion.forward(global_state)

    logger.checkpoint("Analysis session completed")
    logger.elapsed_time("Total analysis time")

    return report, global_state


if __name__ == "__main__":
    args = parse_args()
    main(args)
